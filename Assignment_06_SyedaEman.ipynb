{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: Explain the basic structure and working of a simple Artificial Neural Network (ANN).\n",
        "In your explanation, include the following points:**\n",
        "\n",
        "**●   The role of neurons and layers in an ANN.**\n",
        "\n",
        "**● How information flows through the network.**\n",
        "\n",
        "**● The significance of activation functions and provide examples of commonly used activation functions.**\n",
        "\n",
        "**● The concept of weights and biases and their role in training an ANN**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wOBdAUd2jdeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: ANN stands for Artificial Neural Network.It's a neural network which tries to copy a biological brain. It consists of neurons and layers in order to work. Neurons in ANN process information like recieving inputs, applying weights to inputs, summing the inputs, and passing the results through activation function. Layers of interconnected neurons have specefic roles like the input layer recieves raw data, the hidden layers perform complex computations, and the output layer provides the final result. The data is fed to the input layer and then moves through each hidden layer, where each neuron takes the inputs, performs calculations, and passes the result to the next layer. Finally, the processed data reaches the output layer, which gives the final result.\n",
        "\n",
        "Activation functions play crucial role in a neural network. These functions introduce non-linearit and are applied to the outputs.\n",
        "Some of the most common functions include ReLU (Rectified Linear Unit), Softmax, and Sigmoid.\n",
        "\n",
        "Weights represent the strength of the connection between two neurons and biases are like thresholds that control when a neuron activates. They help the network learn complex patterns.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9QrSqndNkPug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2**"
      ],
      "metadata": {
        "id": "XJVrwiQpIfCO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU1cRZTnsvdd",
        "outputId": "febddb02-5599-4029-80fe-de669da4a4e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 11s 5ms/step - loss: 0.2308 - accuracy: 0.9306 - val_loss: 0.1211 - val_accuracy: 0.9623\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0975 - accuracy: 0.9704 - val_loss: 0.1024 - val_accuracy: 0.9682\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0684 - accuracy: 0.9787 - val_loss: 0.0783 - val_accuracy: 0.9774\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0519 - accuracy: 0.9833 - val_loss: 0.0995 - val_accuracy: 0.9713\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0421 - accuracy: 0.9865 - val_loss: 0.0799 - val_accuracy: 0.9758\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0322 - accuracy: 0.9897 - val_loss: 0.0885 - val_accuracy: 0.9752\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0280 - accuracy: 0.9907 - val_loss: 0.0899 - val_accuracy: 0.9766\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0240 - accuracy: 0.9922 - val_loss: 0.0949 - val_accuracy: 0.9761\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0203 - accuracy: 0.9931 - val_loss: 0.0824 - val_accuracy: 0.9785\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0189 - accuracy: 0.9937 - val_loss: 0.1172 - val_accuracy: 0.9734\n",
            "Test loss: 0.1172391027212143\n",
            "Test accuracy: 0.9733999967575073\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess data (normalize pixel values)\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# Reshape data for input layer (28x28 pixels)\n",
        "x_train = x_train.reshape(x_train.shape[0], 28 * 28)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28 * 28)\n",
        "\n",
        "# One-hot encode labels (optional, but recommended for categorical classification)\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Build the FCNN model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(28 * 28,)))  # Input layer\n",
        "model.add(Dense(128, activation='relu'))  # Hidden layer 1 with ReLU activation\n",
        "model.add(Dense(128, activation='relu'))  # Hidden layer 2 with ReLU activation\n",
        "model.add(Dense(10, activation='softmax'))  # Output layer with softmax activation\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: Explain the importance of hyperparameter tuning in training an ANN. List at least\n",
        "three hyperparameters commonly tuned in ANN training and describe their impact on the\n",
        "model's performance.**"
      ],
      "metadata": {
        "id": "wj1XanUj3TRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In training an ANN, hyperparameter tuning helps you find the best settings to make the model learn well. This can improve its accuracy and prevent overfitting.\n",
        "\n",
        "Learning rate: A high learning rate can make the model learn quickly, but it might miss important patterns or become unstable. A low learning rate can make training slow.\n",
        "\n",
        "Epochs: Too few epochs might not allow the model to learn sufficiently, while too many epochs can lead to overfitting, where the model memorizes the training data but performs poorly on unseen data.\n",
        "\n",
        "Batch size: A larger batch size can improve training efficiency but might lead to rougher updates. A smaller batch size provides more frequent updates but might require more iterations overall."
      ],
      "metadata": {
        "id": "n3FpBjQS3WGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 4**"
      ],
      "metadata": {
        "id": "UoSwISYAIsY0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP9Urc5KAxNQ",
        "outputId": "9a251f64-29d7-47ef-c80a-5abad5f4778c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation Function: relu, Test Accuracy: 1.0000\n",
            "Activation Function: sigmoid, Test Accuracy: 0.9000\n",
            "Activation Function: tanh, Test Accuracy: 1.0000\n",
            "\n",
            "Summary of Results:\n",
            "relu: 1.0000\n",
            "sigmoid: 0.9000\n",
            "tanh: 1.0000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Load and preprocess the data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y = to_categorical(y, 3)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to build and train the model\n",
        "def build_and_train_model(activation_function):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(10, input_shape=(4,), activation=activation_function))\n",
        "    model.add(Dense(10, activation=activation_function))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, epochs=50, batch_size=5, verbose=0)\n",
        "\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    return accuracy\n",
        "\n",
        "# Step 3: Train and evaluate models with different activation functions\n",
        "activations = ['relu', 'sigmoid', 'tanh']\n",
        "results = {}\n",
        "\n",
        "for activation in activations:\n",
        "    accuracy = build_and_train_model(activation)\n",
        "    results[activation] = accuracy\n",
        "    print(f'Activation Function: {activation}, Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Output the results\n",
        "print(\"\\nSummary of Results:\")\n",
        "for activation, accuracy in results.items():\n",
        "    print(f'{activation}: {accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 5**"
      ],
      "metadata": {
        "id": "2yU_u_ReIu8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/Weather Training Data.csv')"
      ],
      "metadata": {
        "id": "qCg3huVaI-Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "wElkFbCE6HLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df['MinTemp'] = imputer.fit_transform(df[['MinTemp']])\n",
        "df['MaxTemp'] = imputer.fit_transform(df[['MaxTemp']])\n",
        "df['Rainfall'] = imputer.fit_transform(df[['Rainfall']])\n",
        "df['WindGustSpeed'] = imputer.fit_transform(df[['WindGustSpeed']])\n",
        "df['WindSpeed9am'] = imputer.fit_transform(df[['WindSpeed9am']])\n",
        "df['WindSpeed3pm'] = imputer.fit_transform(df[['WindSpeed3pm']])\n",
        "df['Humidity9am'] = imputer.fit_transform(df[['Humidity9am']])\n",
        "df['Humidity3pm'] = imputer.fit_transform(df[['Humidity3pm']])\n",
        "df['Pressure3pm'] = imputer.fit_transform(df[['Pressure3pm']])\n",
        "df['Temp9am'] = imputer.fit_transform(df[['Temp9am']])\n",
        "df['Temp3pm'] = imputer.fit_transform(df[['Temp3pm']])\n"
      ],
      "metadata": {
        "id": "9KJlfZS_6HM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['row ID'])"
      ],
      "metadata": {
        "id": "apZtjHxC6PsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing categorical data with mode\n",
        "categorical_columns = ['Location', 'WindDir9am', 'WindDir3pm', 'RainToday']\n",
        "for col in categorical_columns:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "a3KjFlAn6SJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "for col in categorical_columns:\n",
        "    df[col] = label_encoder.fit_transform(df[col])"
      ],
      "metadata": {
        "id": "_yg56CJF6fXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the target variable\n",
        "df['RainTomorrow'] = label_encoder.fit_transform(df['RainTomorrow'])\n"
      ],
      "metadata": {
        "id": "MXDUrTGH6hdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target\n",
        "X = df.drop(columns=['RainTomorrow'])\n",
        "y = df['RainTomorrow']\n"
      ],
      "metadata": {
        "id": "BK5lTJDO6hfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "kZ_vX1XTqBgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all columns in X_train and X_test to numeric, coercing errors\n",
        "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
        "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Fill any missing values (NaN) introduced by coercion with 0\n",
        "X_train = X_train.fillna(0)\n",
        "X_test = X_test.fillna(0)"
      ],
      "metadata": {
        "id": "EhBxxO1DqcpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "owjY3HZqqHKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer and first hidden layer\n",
        "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "\n",
        "# Second hidden layer\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "N9b9ceY_qg39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmsgHLflqzIF",
        "outputId": "26b0490d-2d58-434d-a93a-5cec6392c49a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1991/1991 [==============================] - 7s 3ms/step - loss: 0.3657 - accuracy: 0.8415 - val_loss: 0.3623 - val_accuracy: 0.8395\n",
            "Epoch 2/20\n",
            "1991/1991 [==============================] - 7s 4ms/step - loss: 0.3496 - accuracy: 0.8480 - val_loss: 0.3538 - val_accuracy: 0.8439\n",
            "Epoch 3/20\n",
            "1991/1991 [==============================] - 5s 3ms/step - loss: 0.3451 - accuracy: 0.8499 - val_loss: 0.3511 - val_accuracy: 0.8438\n",
            "Epoch 4/20\n",
            "1991/1991 [==============================] - 7s 3ms/step - loss: 0.3421 - accuracy: 0.8515 - val_loss: 0.3549 - val_accuracy: 0.8418\n",
            "Epoch 5/20\n",
            "1991/1991 [==============================] - 6s 3ms/step - loss: 0.3396 - accuracy: 0.8525 - val_loss: 0.3504 - val_accuracy: 0.8459\n",
            "Epoch 6/20\n",
            "1991/1991 [==============================] - 7s 3ms/step - loss: 0.3371 - accuracy: 0.8539 - val_loss: 0.3492 - val_accuracy: 0.8463\n",
            "Epoch 7/20\n",
            "1991/1991 [==============================] - 6s 3ms/step - loss: 0.3357 - accuracy: 0.8532 - val_loss: 0.3480 - val_accuracy: 0.8473\n",
            "Epoch 8/20\n",
            "1991/1991 [==============================] - 6s 3ms/step - loss: 0.3339 - accuracy: 0.8549 - val_loss: 0.3480 - val_accuracy: 0.8485\n",
            "Epoch 9/20\n",
            "1991/1991 [==============================] - 7s 3ms/step - loss: 0.3323 - accuracy: 0.8552 - val_loss: 0.3509 - val_accuracy: 0.8461\n",
            "Epoch 10/20\n",
            "1991/1991 [==============================] - 7s 3ms/step - loss: 0.3313 - accuracy: 0.8565 - val_loss: 0.3469 - val_accuracy: 0.8478\n",
            "Epoch 11/20\n",
            "1991/1991 [==============================] - 7s 3ms/step - loss: 0.3299 - accuracy: 0.8565 - val_loss: 0.3503 - val_accuracy: 0.8485\n",
            "Epoch 12/20\n",
            "1991/1991 [==============================] - 5s 3ms/step - loss: 0.3287 - accuracy: 0.8571 - val_loss: 0.3514 - val_accuracy: 0.8448\n",
            "Epoch 13/20\n",
            "1991/1991 [==============================] - 7s 3ms/step - loss: 0.3280 - accuracy: 0.8569 - val_loss: 0.3480 - val_accuracy: 0.8481\n",
            "Epoch 14/20\n",
            "1991/1991 [==============================] - 6s 3ms/step - loss: 0.3257 - accuracy: 0.8582 - val_loss: 0.3505 - val_accuracy: 0.8481\n",
            "Epoch 15/20\n",
            "1991/1991 [==============================] - 7s 4ms/step - loss: 0.3252 - accuracy: 0.8584 - val_loss: 0.3487 - val_accuracy: 0.8500\n",
            "Epoch 16/20\n",
            "1991/1991 [==============================] - 6s 3ms/step - loss: 0.3242 - accuracy: 0.8593 - val_loss: 0.3468 - val_accuracy: 0.8483\n",
            "Epoch 17/20\n",
            "1991/1991 [==============================] - 7s 4ms/step - loss: 0.3234 - accuracy: 0.8596 - val_loss: 0.3485 - val_accuracy: 0.8491\n",
            "Epoch 18/20\n",
            "1991/1991 [==============================] - 5s 3ms/step - loss: 0.3221 - accuracy: 0.8597 - val_loss: 0.3494 - val_accuracy: 0.8476\n",
            "Epoch 19/20\n",
            "1991/1991 [==============================] - 7s 4ms/step - loss: 0.3208 - accuracy: 0.8605 - val_loss: 0.3475 - val_accuracy: 0.8491\n",
            "Epoch 20/20\n",
            "1991/1991 [==============================] - 11s 5ms/step - loss: 0.3203 - accuracy: 0.8616 - val_loss: 0.3527 - val_accuracy: 0.8478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "test_data = pd.read_csv('/content/Weather Test Data.csv')"
      ],
      "metadata": {
        "id": "FaMqCH5s7dEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "aM6qUheWDjru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "for col in categorical_columns:\n",
        "    test_data[col] = label_encoder.fit_transform(test_data[col])"
      ],
      "metadata": {
        "id": "wdDYYsWKEESS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target\n",
        "X = test_data.drop(columns=['RainToday'])\n",
        "y = test_data['RainToday']"
      ],
      "metadata": {
        "id": "4rFeHv_kEGTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all columns in X_train and X_test to numeric, coercing errors\n",
        "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
        "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Fill any missing values (NaN) introduced by coercion with 0\n",
        "X_train = X_train.fillna(0)\n",
        "X_test = X_test.fillna(0)"
      ],
      "metadata": {
        "id": "7_jCmMrZFhDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Test Mean Squared Error: {mse:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzQnRk6VDXJc",
        "outputId": "fad6cb6a-7c15-459a-b3dc-cc997cb6d8bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "622/622 [==============================] - 1s 2ms/step - loss: 121.4932 - accuracy: 0.7241\n",
            "Test Accuracy: 0.72\n",
            "622/622 [==============================] - 1s 1ms/step\n",
            "Test Mean Squared Error: 0.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example new data (assuming it has been preprocessed in the same way)\n",
        "new_data = pd.DataFrame({\n",
        "    'Location': [0],  # Example values\n",
        "    'MinTemp': [14.0],\n",
        "    'MaxTemp': [25.0],\n",
        "    'Rainfall': [0.0],\n",
        "    'Evaporation': [5.4], # Added missing columns\n",
        "    'Sunshine': [7.6],\n",
        "    'WindGustDir': [8], # Added missing 'WindGustDir' feature, replace 8 with actual direction\n",
        "    'WindGustSpeed': [35],\n",
        "    'WindDir9am': [8],\n",
        "    'WindDir3pm': [16],\n",
        "    'WindSpeed9am': [7],\n",
        "    'WindSpeed3pm': [19],\n",
        "    'Humidity9am': [68],\n",
        "    'Humidity3pm': [55],\n",
        "    'Pressure9am': [1012],\n",
        "    'Pressure3pm': [1015],\n",
        "    'Cloud9am': [7],\n",
        "    'Cloud3pm': [8],\n",
        "    'Temp9am': [20.0],\n",
        "    'Temp3pm': [23.0],\n",
        "    'RainToday': [0]\n",
        "})\n",
        "\n",
        "# Ensure the columns are in the same order as during training\n",
        "# Replace with the actual column order from your training data\n",
        "original_columns = ['Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine',\n",
        "                   'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'WindSpeed9am', # Added 'WindGustDir' here\n",
        "                   'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am',\n",
        "                   'Cloud3pm', 'Temp9am', 'Temp3pm', 'RainToday']\n",
        "new_data = new_data[original_columns]\n",
        "\n",
        "# Scale the new data\n",
        "new_data = scaler.transform(new_data)\n",
        "\n",
        "# Predict\n",
        "new_prediction = model.predict(new_data)\n",
        "new_prediction = (new_prediction > 0.5).astype(int)\n",
        "print(f'Predicted Rain Today: {\"Yes\" if new_prediction[0][0] == 1 else \"No\"}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOjy6zgmFq1S",
        "outputId": "5ab3862f-fdb1-4b31-d035-aa680736a91e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted Rain Today: No\n"
          ]
        }
      ]
    }
  ]
}
