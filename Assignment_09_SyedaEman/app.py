import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema.output_parser import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.document_loaders import PyPDFLoader
from streamlit_mic_recorder import speech_to_text
from gtts import gTTS
import os

# Define your API key (replace with actual API key)
api_key = "AIzaSyCxQtYjg28vBKICtZweHIunusmIPRi5Tts"

# Create a chat template with prompt
chat_template = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful AI assistant. Please always respond to user queries in Pure Urdu language and use the context from the PDF if available.",
        ),
        ("human", "{human_input}"),
    ]
)

# Initialize the model with the Google Generative AI
model = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=api_key)

# Create the chain by combining the template, model, and output parser
chain = chat_template | model | StrOutputParser()

# Streamlit interface
st.set_page_config(page_title="Ø§Ø±Ø¯Ùˆ Ø³ÙˆØ§Ù„ Ùˆ Ø¬ÙˆØ§Ø¨ Ø¨ÙˆÙ¹", page_icon="ğŸ¤–")
st.title("Ø§Ø±Ø¯Ùˆ Ø³ÙˆØ§Ù„ Ùˆ Ø¬ÙˆØ§Ø¨ Ø¨ÙˆÙ¹")
st.write("Ø¢Ù¾ Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ø³ÙˆØ§Ù„ Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø¬ÙˆØ§Ø¨Ø§Øª Ø­Ø§ØµÙ„ Ú©Ø±ÛŒÚºÛ”")

# PDF file upload
uploaded_file = st.file_uploader("PDF ÙØ§Ø¦Ù„ Ø§Ù¾ Ù„ÙˆÚˆ Ú©Ø±ÛŒÚº", type=["pdf"])
if uploaded_file is not None:
    # Save the uploaded file to a temporary location
    temp_file_path = "temp_pdf.pdf"
    with open(temp_file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    # Load the PDF file
    pdf_loader = PyPDFLoader(temp_file_path)
    documents = pdf_loader.load()

    # Speech input section
    st.subheader("Ø§Ù¾Ù†Ø§ Ø³ÙˆØ§Ù„ Ø¨ÙˆÙ„ÛŒÚº:")
    speech_input = speech_to_text(language="ur", use_container_width=True, just_once=True, key="STT")

    # Text input for user query
    user_input = st.text_input("ÛŒØ§ Ø§Ù¾Ù†Ø§ Ø³ÙˆØ§Ù„ ÛŒÛØ§Úº Ù„Ú©Ú¾ÛŒÚº (Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº):", placeholder="Ø§Ù¾Ù†Ø§ Ø³ÙˆØ§Ù„ ÛŒÛØ§Úº Ù„Ú©Ú¾ÛŒÚº...")

    # Combine inputs if both are provided
    final_input = speech_input if speech_input else user_input

    if final_input:
        st.write(f"Ø¢Ù¾ Ú©Ø§ Ø³ÙˆØ§Ù„: **{final_input}**")  # Display the recognized or typed text in Urdu
        with st.spinner("Ø¬ÙˆØ§Ø¨ ØªÛŒØ§Ø± Ú©ÛŒØ§ Ø¬Ø§ Ø±ÛØ§ ÛÛ’..."):
            # Combine the context from the PDF and the user's question
            context = "\n".join([doc.page_content for doc in documents])  # Extract text from documents
            full_input = f"{context}\n\nUser Question: {final_input}"

            # Invoke the model with the combined input
            res = chain.invoke({"human_input": full_input})

            # Ensure the response is in Urdu
            if not res:
                st.error("Ø¬ÙˆØ§Ø¨ Ù¾ÛŒØ¯Ø§ Ù†ÛÛŒÚº Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©Ø§Û” Ø¨Ø±Ø§Û Ú©Ø±Ù… Ø¯ÙˆØ¨Ø§Ø±Û Ú©ÙˆØ´Ø´ Ú©Ø±ÛŒÚºÛ”")
            else:
                # Display the response text
                st.subheader("Ø¬ÙˆØ§Ø¨:")
                st.write(f"**{res}**")

                # Convert response text to speech using gTTS
                tts = gTTS(text=res, lang='ur')
                tts_file = "response.mp3"
                tts.save(tts_file)

                # Play the audio response
                with open(tts_file, "rb") as audio_file:
                    audio_bytes = audio_file.read()
                    st.audio(audio_bytes, format="audio/mp3")

                # Clean up the audio file after playing
                os.remove(tts_file)

    # Clean up the temporary PDF file
    os.remove(temp_file_path)
else:
    st.info("Ø¨Ø±Ø§Û Ú©Ø±Ù… PDF ÙØ§Ø¦Ù„ Ø§Ù¾ Ù„ÙˆÚˆ Ú©Ø±ÛŒÚºÛ”")

